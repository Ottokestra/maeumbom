import json
import os
import logging
from typing import Any, Dict, Optional
from datetime import datetime

from langchain_openai import ChatOpenAI
from openai import OpenAI

from pydantic import BaseModel

from .db_conversation_store import get_conversation_store
from .orchestrator import orchestrator_llm, execute_tools
from .emotion_classifier import get_emotion_classifier
from .response_generator import (
    get_casual_tone_system_prompt,
    generate_response_metadata,
    parse_alarm_request,
)
from app.db.database import SessionLocal

logger = logging.getLogger(__name__)


class CharacterInfo(BaseModel):
    id: str
    emotion_label: str


def select_character_from_emotion(emotion_result: dict[str, Any]) -> tuple[str, str]:
    """
    emotion_result ì—ì„œ ì£¼ìš” ê°ì •/í´ëŸ¬ìŠ¤í„°ë¥¼ ì½ì–´ì„œ ìºë¦­í„° IDì™€ ê°ì • ë¼ë²¨ì„ ê²°ì •í•œë‹¤.

    ë°˜í™˜ê°’:
        (character_id, emotion_label)
    ì˜ˆ:
        ("sunny_flower", "JOY")
        ("sad_cloud", "SADNESS")
    """
    primary = (
        emotion_result.get("primary_emotion")
        or emotion_result.get("cluster")
        or "NEUTRAL"
    )

    mapping = {
        "JOY": "sunny_flower",  # í•´ë°”ë¼ê¸°
        "HAPPINESS": "sunny_flower",
        "LOVE": "heart_cat",  # í•˜íŠ¸ëˆˆ ê³ ì–‘ì´
        "HOPE": "star_friend",  # ë³„
        "INSIGHT": "idea_bulb",  # ì „êµ¬
        "ANGER": "fire_spirit",  # ë¶ˆ
        "SADNESS": "rain_cloud",  # ìš°ëŠ” êµ¬ë¦„
        "FEAR": "ghost_white",  # ìœ ë ¹
        "ANXIETY": "stone_gray",  # ëŒ
        "TIRED": "sloth_bear",  # ë‚˜ë¬´ëŠ˜ë³´
        "STRESS": "devil_red",  # ì•…ë§ˆ
        "CONFUSION": "alien_green",  # ì™¸ê³„ì¸
        # ê¸°ë³¸ê°’
        "NEUTRAL": "cloud_white",
    }

    normalized = str(primary).upper()
    character_id = mapping.get(normalized, "cloud_white")
    return character_id, normalized


async def run_ai_bomi_from_text(
    user_text: str,
    user_id: int,
    session_id: str = "default",
    stt_quality: str = "success",
    speaker_id: Optional[str] = None,
) -> dict[str, Any]:
    """í…ìŠ¤íŠ¸ ì…ë ¥ ê¸°ë°˜ AI ë´„ì´ ì‹¤í–‰ (orchestrator ê¸°ë°˜)"""
    
    store = get_conversation_store()
    db_session = SessionLocal()
    
    try:
        # 1. Lightweight Classifierë¡œ ê°ì • ë¶„ì„ í•„ìš” ì—¬ë¶€ íŒë‹¨
        classifier = get_emotion_classifier()
        classifier_hint = classifier.predict(user_text)
        logger.info(f"ğŸ¯ [Classifier] Result: {classifier_hint}")
        
        # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        history = store.get_history(user_id, session_id, limit=10)
        conversation_history = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in history
        ]
        
        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = {
            "session_id": session_id,
            "user_id": user_id,
            "history": conversation_history,
            "memory": "",  # TODO: ë©”ëª¨ë¦¬ í†µí•©
        }
        
        # 4. Orchestratorë¡œ ë„êµ¬ ì„ íƒ
        tool_calls = await orchestrator_llm(
            user_text=user_text,
            context=context,
            classifier_hint=classifier_hint
        )
        
        # 5. ë„êµ¬ ì‹¤í–‰
        tool_results = await execute_tools(
            tool_calls=tool_calls,
            user_id=user_id,
            session_id=session_id,
            user_text=user_text,
            db_session=db_session
        )
        
        # 6. LLM ì‘ë‹µ ìƒì„±
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        system_prompt = get_casual_tone_system_prompt()
        
        # ë„êµ¬ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        tool_context = ""
        if tool_results.get("emotion"):
            emotion_data = tool_results["emotion"].get("result") or tool_results["emotion"]
            tool_context += f"\n[ê°ì • ë¶„ì„ ê²°ê³¼]\n{json.dumps(emotion_data, ensure_ascii=False, indent=2)}\n"
        if tool_results.get("routines"):
            tool_context += f"\n[ì¶”ì²œ ë£¨í‹´]\n{json.dumps(tool_results['routines'], ensure_ascii=False, indent=2)}\n"
        
        messages = [
            {"role": "system", "content": system_prompt + tool_context}
        ]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        for msg in conversation_history[-5:]:  # ìµœê·¼ 5ê°œë§Œ
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        messages.append({"role": "user", "content": user_text})
        
        # LLM í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7,
        )
        
        reply_text = response.choices[0].message.content.strip()
        
        # 7. ì‘ë‹µ ë©”íƒ€ë°ì´í„° ìƒì„±
        response_metadata = generate_response_metadata(
            conversation_history=conversation_history,
            llm_response=reply_text,
            user_text=user_text
        )
        
        # 8. ì•ŒëŒ íŒŒì‹±
        alarm_data = parse_alarm_request(
            user_text=user_text,
            llm_response=reply_text,
            current_datetime=datetime.now()
        )
        
        # 9. ê°ì • ê²°ê³¼ ì¶”ì¶œ
        emotion_result = None
        if tool_results.get("emotion"):
            emotion_data = tool_results["emotion"]
            if isinstance(emotion_data, dict):
                emotion_result = emotion_data.get("result") or emotion_data
        
        # 10. DBì— ì €ì¥
        store.add_message(
            user_id=user_id,
            session_id=session_id,
            role="user-A",
            content=user_text
        )
        store.add_message(
            user_id=user_id,
            session_id=session_id,
            role="assistant",
            content=reply_text
        )
        
        # 11. ê²°ê³¼ ë°˜í™˜
        return {
            "reply_text": reply_text,
            "input_text": user_text,
            "emotion_result": emotion_result,
            "routine_result": tool_results.get("routines"),
            "meta": {
                "model": "gpt-4o-mini",
                "used_tools": [tc.function.name for tc in tool_calls],
                "session_id": session_id,
                "stt_quality": stt_quality,
                "user_id": user_id,
                "storage": "database",
                "api_version": "v2",
                "emotion": response_metadata.get("emotion"),
                "response_type": response_metadata.get("response_type"),
                "alarm": alarm_data if alarm_data.get("response_type") == "alarm" else None,
            },
        }
        
    finally:
        db_session.close()


async def run_ai_bomi_from_text_v2(
    user_text: str,
    user_id: int,
    session_id: str = "default",
    stt_quality: str = "success",
    speaker_id: Optional[str] = None,
) -> dict[str, Any]:
    """í…ìŠ¤íŠ¸ ì…ë ¥ ê¸°ë°˜ AI ë´„ì´ ì‹¤í–‰ (v2: ìºë¦­í„° ì •ë³´ í¬í•¨)"""

    base_result = await run_ai_bomi_from_text(
        user_text=user_text,
        user_id=user_id,
        session_id=session_id,
        stt_quality=stt_quality,
        speaker_id=speaker_id,
    )

    emotion_result = {}
    if isinstance(base_result.get("emotion_result"), dict):
        emotion_result = base_result.get("emotion_result", {})

    character_id, emotion_label = select_character_from_emotion(emotion_result)

    meta = base_result.get("meta") or {}

    return {
        **base_result,
        "character": {
            "id": character_id,
            "emotion_label": emotion_label,
        },
        "meta": {
            **meta,
            "tts_engine_default": "cute_bomi",
        },
    }


def _get_llm_client() -> ChatOpenAI:
    model_name = os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini")
    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return ChatOpenAI(
        model=model_name,
        temperature=0.6,
        api_key=api_key,
    )


def _serialize_weekly_summary(weekly_summary: Any) -> Dict[str, Any]:
    if hasattr(weekly_summary, "model_dump"):
        return weekly_summary.model_dump()
    if hasattr(weekly_summary, "dict"):
        return weekly_summary.dict()
    return dict(weekly_summary)


def generate_weekly_emotion_report_story(weekly_summary: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê¸°ì¡´ ì£¼ê°„ ê°ì • ë¦¬í¬íŠ¸ ë°ì´í„°ë¥¼ ë°›ì•„ ìºë¦­í„° ë§í’ì„  ë¦¬í¬íŠ¸ë¡œ ë³€í™˜í•œë‹¤.

    Args:
        weekly_summary: ê¸°ì¡´ ì£¼ê°„ ê°ì • ë¦¬í¬íŠ¸ ì„œë¹„ìŠ¤ê°€ ë°˜í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°

    Returns:
        EmotionReportChatResponse ìŠ¤í‚¤ë§ˆì— ë§ëŠ” dict
    """

    llm = _get_llm_client()
    serialized_summary = _serialize_weekly_summary(weekly_summary)

    prompt = f"""
    ë„ˆëŠ” í•œêµ­ì–´ ê°ì •ì½”ì¹˜ 'ë´„ì´'ì•¼.
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§€ë‚œ ì¼ì£¼ì¼ ê°ì • ìš”ì•½ ë°ì´í„°ì•¼.
    ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ í˜•ì‹ì˜ JSON ì„ ë§Œë“¤ì–´ë¼.

    - headline: "ì´ë²ˆ ì£¼ì˜ ë„ˆëŠ” 'OOO'" í˜•íƒœì˜ í•œ ì¤„ ì œëª©
    - character:
        - key: í”„ë¡ íŠ¸ì—ì„œ ì‚¬ìš©í•  ìºë¦­í„° í‚¤ (ex: worried_fox, happy_star ë“±)
        - display_name: ì‚¬ìš©ìë¥¼ ìƒì§•í•˜ëŠ” ìºë¦­í„° ì´ë¦„ (í•œêµ­ì–´)
        - mood: ì˜ì–´ í•œ ë‹¨ì–´ ê°ì • ë ˆì´ë¸” (happy, sad, worry, angry ë“±)
    - bubbles: ìºë¦­í„°ì™€ ì‚¬ìš©ì ê°„ì˜ ì§§ì€ ëŒ€í™” ë§í’ì„  ë¦¬ìŠ¤íŠ¸.
        - ì²« ë§ì€ í•­ìƒ ìºë¦­í„°ê°€ ì‚¬ìš©ìë¥¼ ë¶€ë“œëŸ½ê²Œ ë§ì´í•˜ëŠ” ì¸ì‚¬
        - ì´ 4~6ê°œ ì •ë„
        - role ì€ "character" ë˜ëŠ” "user"

    ê°ì • ìš”ì•½ ë°ì´í„°:
    {serialized_summary}

    ë°˜ë“œì‹œ ì•„ë˜ Python dict í˜•íƒœë¡œë§Œ ì‘ë‹µí•´.
    keys: headline, character, bubbles
    """

    try:
        response = llm.invoke(prompt)
        content = getattr(response, "content", "")
        parsed = json.loads(content)
    except Exception:
        parsed = {}

    headline = parsed.get(
        "headline", serialized_summary.get("summary_title", "ì´ë²ˆ ì£¼ì˜ ë„ˆëŠ” 'ë´„ì´ì™€ í•¨ê»˜í•œ ì¹œêµ¬'"),
    )
    character = parsed.get(
        "character",
        {"key": "calm_bomi", "display_name": "ë´„ì´", "mood": "calm"},
    )
    bubbles = parsed.get(
        "bubbles",
        [
            {
                "role": "character",
                "text": "ì´ë²ˆ ì£¼ì—ë„ ì˜ ë²„í…¨ì¤˜ì„œ ê³ ë§ˆì›Œ. ê¸°ë¶„ì€ ì–´ë• ì–´?",
            },
            {
                "role": "user",
                "text": "ê¸°ë³µì´ ìˆì—ˆì§€ë§Œ ë‚˜ë¦„ ê´œì°®ì•˜ì–´.",
            },
            {
                "role": "character",
                "text": "ê·¸ë˜ë„ ìŠ¤ìŠ¤ë¡œë¥¼ ì±™ê¸°ë ¤ëŠ” ë…¸ë ¥ì´ ë³´ì—¬ì„œ ë“ ë“ í•´!",
            },
        ],
    )

    summary_stats = {
        "dominant_emotion": serialized_summary.get("dominant_emotion"),
        "week_range": {
            "start": serialized_summary.get("week_start"),
            "end": serialized_summary.get("week_end"),
        },
    }

    return {
        "period": "weekly",
        "headline": headline,
        "character": character,
        "bubbles": bubbles,
        "summary_stats": summary_stats,
    }
