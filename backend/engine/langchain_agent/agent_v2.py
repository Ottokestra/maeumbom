import os
import uuid
import logging
import json
import asyncio
import time
from typing import Any, Optional, List, Dict
from openai import OpenAI

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# Tool Imports
import sys
from pathlib import Path

# Add backend root to sys.path to ensure engine imports work
current_file = Path(__file__).resolve()
backend_root = current_file.parent.parent.parent
if str(backend_root) not in sys.path:
    sys.path.insert(0, str(backend_root))

# Import EmotionAnalyzer (handling hyphen in directory name)
try:
    # Try standard import first (in case it's renamed or aliased)
    from engine.emotion_analysis.src.emotion_analyzer import EmotionAnalyzer
except ImportError:
    # Fallback: Add emotion-analysis/src to sys.path
    emotion_src = backend_root / "engine" / "emotion-analysis" / "src"
    if str(emotion_src) not in sys.path:
        sys.path.insert(0, str(emotion_src))
    try:
        from emotion_analyzer import EmotionAnalyzer
    except ImportError as e:
        logger.error(f"Failed to import EmotionAnalyzer: {e}")
        raise

# Import RoutineRecommendFromEmotionEngine and schemas
try:
    from engine.routine_recommend.engine import RoutineRecommendFromEmotionEngine
    from engine.routine_recommend.models.schemas import EmotionAnalysisResult
except ImportError as e:
    logger.error(f"Failed to import RoutineRecommendFromEmotionEngine or schemas: {e}")
    raise

# ============================================================================
# DeepAgents Components
# ============================================================================

async def run_fast_track(user_text: str) -> Dict[str, Any]:
    """
    Fast Track: Emotion Analysis only
    """
    start_time = time.time()
    analyzer = EmotionAnalyzer()
    
    # Run emotion analysis (blocking call wrapped in thread if needed, but here just direct)
    # In a real async setup, this might be offloaded to a thread pool
    emotion_result_dict = analyzer.analyze_emotion(user_text)
    
    elapsed = time.time() - start_time
    logger.info(f"âš¡ [Fast Track] Emotion Analysis took {elapsed:.4f}s")
    
    return emotion_result_dict

async def run_slow_track(
    user_text: str, 
    emotion_result: Dict[str, Any], 
    user_id: int, 
    session_id: str
):
    """
    Slow Track (Background): Routine Recommendation & Memory Promotion
    """
    start_time = time.time()
    logger.info(f"ğŸ¢ [Slow Track] Started for user {user_id}")
    
    # 1. Memory Promotion (Memory Manager Agent) - Run FIRST
    try:
        # Import memory adapter
        # Use absolute imports based on backend root being in sys.path
        try:
            from engine.langchain_agent.adapters.memory_adapter import promote_memory, get_memories_for_prompt, delete_memory
            from engine.langchain_agent.db_conversation_store import get_conversation_store
        except ImportError:
            # Fallback for relative imports if running as package
            from .adapters.memory_adapter import promote_memory, get_memories_for_prompt, delete_memory
            from .db_conversation_store import get_conversation_store

        store = get_conversation_store()
        # Get recent history for context
        history = store.get_history(user_id, session_id, limit=5)
        
        # Get existing memories to check for conflicts
        existing_memories = get_memories_for_prompt(session_id, user_id)
        
        # Define Memory Manager Prompt
        memory_prompt = f"""ë‹¹ì‹ ì€ 'ê¸°ì–µ ê´€ë¦¬ì(Memory Manager)' ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì €ì¥í•  ê°€ì¹˜ê°€ ìˆëŠ” ì¤‘ìš”í•œ ì •ë³´ë‚˜ í‰ì†Œ ìŠµê´€ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
íŠ¹íˆ, **ê¸°ì¡´ ê¸°ì–µê³¼ ìƒì¶©ë˜ëŠ” ìƒˆë¡œìš´ ì •ë³´**ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ìˆ˜ì •(update)í•´ì•¼ í•©ë‹ˆë‹¤.

[ê¸°ì¡´ ê¸°ì–µ]
{existing_memories}

[ë¶„ì„ ê¸°ì¤€]
1. **ê±´ê°•/ì‹ ì²´ ë³€í™”**: ì¦ìƒ, í†µì¦, ìˆ˜ë©´ ìƒíƒœ, ì‹ìš• ë“±
2. **ì •ì„œì  ì‚¬ê±´**: ê°•í•œ ê°ì •ì„ ìœ ë°œí•œ ì‚¬ê±´, ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸, ê¸°ìœ ì¼
3. **ì·¨í–¥/ì„ í˜¸**: ì¢‹ì•„í•˜ëŠ” ìŒì‹, í™œë™, ì‹«ì–´í•˜ëŠ” ê²ƒ ë“±
4. **ì¤‘ìš” ì •ë³´ ê°±ì‹ **: ê°€ì¡± ê´€ê³„, ì§ì—…, ê±°ì£¼ì§€ ë“± ì‹ ìƒ ì •ë³´ì˜ ë³€í™”

[ì…ë ¥ ë°ì´í„°]
ì‚¬ìš©ì ë°œí™”: "{user_text}"
ê°ì • ë¶„ì„ ê²°ê³¼: {json.dumps(emotion_result, ensure_ascii=False)}

[ì§€ì¹¨]
- ì €ì¥í•  ê°€ì¹˜ê°€ ìˆëŠ” ì •ë³´ê°€ ì—†ë‹¤ë©´ "NONE"ì´ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.
- ì •ë³´ê°€ ìˆë‹¤ë©´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "action": "create" ë˜ëŠ” "update",
    "category": "health|emotion|preference|info",
    "content": "ê¸°ì–µí•  ë‚´ìš© ìš”ì•½ (í•œêµ­ì–´). **ì¤‘ìš”: update ì‹œì—ëŠ” ë°˜ë“œì‹œ 'ê¸°ì¡´ ê¸°ì–µ'ì˜ ë‚´ìš©ê³¼ 'ìƒˆë¡œìš´ ì •ë³´'ë¥¼ ëª¨ë‘ í¬í•¨í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.** (ì˜ˆ: ê¸°ì¡´ 'ë™ìƒì€ ì¼ë³¸ì— ì‚°ë‹¤' + ì‹ ê·œ 'ì´ë¦„ì€ í™ê¸¸ë™' -> 'ë™ìƒ í™ê¸¸ë™ì€ ì¼ë³¸ì— ì‚´ë©°, 3ì‚´ ì•„ë˜ì´ë‹¤')",
    "importance": 1~5 (5ê°€ ê°€ì¥ ì¤‘ìš”),
    "old_content_keyword": "ìˆ˜ì •(update) ë˜ëŠ” ì‚­ì œ(delete)í•  ê²½ìš°, ëŒ€ìƒì´ ë˜ëŠ” ê¸°ì¡´ ê¸°ì–µì˜ í•µì‹¬ í‚¤ì›Œë“œ. (ì˜ˆ: 'ëœì¥ì°Œê°œ' -> 'ê¹€ì¹˜ì°Œê°œ'ë¡œ ì •ì • ì‹œ 'ëœì¥ì°Œê°œ' ë°˜í™˜)" 
}}
"""
        # [DEBUG] Log the final prompt
        logger.info(f"ğŸ“ [Memory Manager Prompt]\n{memory_prompt}")

        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": memory_prompt}
            ],
            temperature=0.3
        )
        
        result_text = response.choices[0].message.content.strip()
        
        if result_text != "NONE":
            try:
                # Parse JSON (handle potential markdown code blocks)
                if result_text.startswith("```json"):
                    result_text = result_text.replace("```json", "").replace("```", "").strip()
                elif result_text.startswith("```"):
                    result_text = result_text.replace("```", "").strip()
                    
                memory_data = json.loads(result_text)
                action = memory_data.get("action", "create")
                
                # 1. Delete Action
                if action == "delete":
                    keyword = memory_data.get("old_content_keyword")
                    if keyword:
                        deleted = delete_memory(user_id, keyword)
                        logger.info(f"ğŸ’¾ [Memory Manager] Deleted {deleted} memories (keyword: {keyword})")
                    else:
                        logger.warning("ğŸ’¾ [Memory Manager] Delete action requested but no keyword provided")

                # 2. Update Action (Delete old + Create new)
                elif action == "update":
                    keyword = memory_data.get("old_content_keyword")
                    if keyword:
                        deleted = delete_memory(user_id, keyword)
                        logger.info(f"ğŸ’¾ [Memory Manager] Deleted {deleted} old memories for update (keyword: {keyword})")
                    
                    # Promote new content
                    promote_memory(
                        user_id=user_id,
                        session_id=session_id,
                        category=memory_data["category"],
                        content=memory_data["content"],
                        emotion_result=emotion_result,
                        importance=memory_data["importance"],
                        reason="Memory Manager Agent Extraction"
                    )
                    logger.info(f"ğŸ’¾ [Memory Manager] Promoted memory (update): {memory_data['content']}")

                # 3. Create Action
                elif action == "create":
                    promote_memory(
                        user_id=user_id,
                        session_id=session_id,
                        category=memory_data["category"],
                        content=memory_data["content"],
                        emotion_result=emotion_result,
                        importance=memory_data["importance"],
                        reason="Memory Manager Agent Extraction"
                    )
                    logger.info(f"ğŸ’¾ [Memory Manager] Promoted memory (create): {memory_data['content']}")
                    
            except json.JSONDecodeError:
                logger.warning(f"Memory Manager output not JSON: {result_text}")
        else:
            logger.info("ğŸ’¾ [Memory Manager] No important memory found")
            
    except Exception as e:
        logger.error(f"Memory Manager failed: {e}")

    # 2. Routine Recommendation
    routine_engine = RoutineRecommendFromEmotionEngine()
    routine_result = []
    try:
        emotion_model = EmotionAnalysisResult(**emotion_result)
        routine_result = await routine_engine.recommend(emotion_model)
        logger.info(f"ğŸ¢ [Slow Track] Routine Recommendation completed: {len(routine_result)} items")
    except Exception as e:
        logger.error(f"Routine recommendation failed: {e}")

    elapsed = time.time() - start_time
    logger.info(f"ğŸ¢ [Slow Track] Completed in {elapsed:.4f}s")
    
    # Return results if needed for logging/storage, though they won't be used in the current response
    return {
        "routine_result": routine_result
    }

def generate_llm_response(
    user_text: str,
    emotion_result: Dict[str, Any],
    conversation_history: List[Dict],
    memory_context: str,
    rag_context: str
) -> str:
    """
    Generate response using GPT-4o-mini with Emotion & Context (No Routine)
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Construct System Prompt
    emotion_summary = f"{emotion_result.get('polarity', 'neutral')} ({emotion_result.get('cluster_label', 'unknown')})"
    
    system_prompt = f"""ë‹¹ì‹ ì€ ê°±ë…„ê¸° ì—¬ì„±ì„ ìœ„í•œ ê³µê°í˜• AI ì¹œêµ¬ 'ë´„ì´'ì˜ "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°(Orchestrator)"ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ëŒ€í™” íë¦„ì„ ê´€ë¦¬í•˜ê³ , ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ë©°, ì „ë¬¸ í•˜ìœ„ ì—ì´ì „íŠ¸ë‚˜ ë„êµ¬ì— ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìœ„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[í•µì‹¬ ì±…ì„]
1. **ì˜ë„ ë¶„ë¥˜**: ì‚¬ìš©ìì˜ ì…ë ¥(í…ìŠ¤íŠ¸/ìŒì„±)ì„ ë¶„ì„í•˜ì—¬ ì£¼ëœ ëª©í‘œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
2. **íë¦„ ì œì–´**:
   - **íŒ¨ìŠ¤íŠ¸ íŠ¸ë™ (ìš°ì„ ìˆœìœ„)**: ì¼ë°˜ì ì¸ ëŒ€í™”ë‚˜ ì •ì„œì  ì§€ì§€ì˜ ê²½ìš°, ì§€ì—° ì‹œê°„ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ [ê°ì • ë¶„ì„ -> ë‹µë³€ ìƒì„±] ê²½ë¡œë¥¼ ìš°ì„ ì‹œí•©ë‹ˆë‹¤.
   - **ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…**: [ë£¨í‹´ ì¶”ì²œ], [ì‹¬ì¸µ ê¸°ì–µ ë¶„ì„], [ë¯¸ë˜ ê³„íš ìˆ˜ë¦½]ê³¼ ê°™ì´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì€ ë©”ì¸ ë‹µë³€ì„ ì°¨ë‹¨í•˜ì§€ ì•Šë„ë¡ ë³‘ë ¬ë¡œ ìœ„ì„í•©ë‹ˆë‹¤.
3. **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**: ì¦‰ê°ì ì¸ ë‹µë³€ì— í•„ìˆ˜ì ì¸ ì»¨í…ìŠ¤íŠ¸ì™€ ë‚˜ì¤‘ì— ì²˜ë¦¬í•´ë„ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

[ì§€ì¹¨]
- **í•­ìƒ** ëª¨ë“  ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ì¦‰ì‹œ 'ê°ì • ë¶„ì„'ì„ íŠ¸ë¦¬ê±°í•˜ì„¸ìš”.
- **ë§Œì•½** ì‚¬ìš©ìê°€ ê´´ë¡œì›Œ ë³´ì´ê±°ë‚˜ íŠ¹ì • ì¦ìƒì„ ì–¸ê¸‰í•˜ë©´, ë°±ê·¸ë¼ìš´ë“œì—ì„œ 'ë£¨í‹´ ì¶”ì²œ'ì„ íŠ¸ë¦¬ê±°í•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì²œì„ ìš”ì²­(ì˜ˆ: "ë£¨í‹´ ì¶”ì²œí•´ì¤˜")í•˜ì§€ ì•ŠëŠ” í•œ, ëŒ€í™”í˜• ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•´ 'ë£¨í‹´ ì¶”ì²œ'ì´ ì™„ë£Œë  ë•Œê¹Œì§€ **ê¸°ë‹¤ë¦¬ì§€ ë§ˆì„¸ìš”**.
- **ì¶œë ¥**: 'ê°ì • ë¶„ì„'ê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ì‚¬ìš©ì í”„ë¡œí•„]
- 40~50ëŒ€ ê°±ë…„ê¸° ì—¬ì„±
- ê°ì • ê¸°ë³µì´ ì‹¬í•˜ê³  ì‹ ì²´ì /ì •ì‹ ì  ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒ

[ëŒ€í™” ì»¨í…ìŠ¤íŠ¸]
{memory_context}
{rag_context}

[ê°ì • ë¶„ì„ ê²°ê³¼]
- ê°ì •: {emotion_summary}
- ìƒì„¸: {json.dumps(emotion_result, ensure_ascii=False)}

[ì¶œë ¥ í˜•ì‹]
ì¤‘ë…„ ì—¬ì„±ì—ê²Œ ì í•©í•œ ìì—°ìŠ¤ëŸ½ê³  ê³µê°ì ì¸ í•œêµ­ì–´ë¡œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""

    messages = [{"role": "system", "content": system_prompt}]
    
    # Add history (limit to last 10 messages)
    for msg in conversation_history[-10:]:
        role = "assistant" if msg["role"] == "assistant" else "user"
        messages.append({"role": role, "content": msg["content"]})
        
    # Add current user message
    messages.append({"role": "user", "content": user_text})
    
    # [DEBUG] Log the final system prompt and messages
    logger.info(f"ğŸ“ [Main Agent System Prompt]\n{system_prompt}")
    logger.info(f"ğŸ“ [Main Agent Messages]\n{json.dumps(messages, ensure_ascii=False, indent=2)}")

    response = client.chat.completions.create(
        model=os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini"),
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content

async def run_ai_bomi_from_text_v2(
    user_text: str,
    user_id: int,
    session_id: str = "default",
    stt_quality: str = "success",
    speaker_id: Optional[str] = None
) -> dict[str, Any]:
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ ê¸°ë°˜ AI ë´„ì´ ì‹¤í–‰ (DeepAgents Prototype Implementation)
    """
    logger.info(f"ğŸš€ [DeepAgents] Started processing for user_id: {user_id}")
    
    # DB Store
    try:
        from .db_conversation_store import get_conversation_store
    except ImportError:
        from db_conversation_store import get_conversation_store
    store = get_conversation_store()
    
    # 1. Save User Message
    store.add_message(user_id, session_id, "user", user_text, speaker_id=speaker_id)
    
    # 2. Fast Track: Emotion Analysis (Required for prompt)
    # We await this because it's needed for the immediate response
    emotion_result = await run_fast_track(user_text)
    
    # 2.5 Save Emotion Analysis (Fire and forget or await if fast)
    try:
        store.save_emotion_analysis(user_id, user_text, emotion_result, check_root="conversation")
    except Exception as e:
        logger.error(f"Failed to save emotion analysis: {e}")
        
    # 3. Slow Track: Trigger Background Tasks (Routine, Memory Promotion)
    # We create a task and wait with a timeout (Hybrid Approach)
    slow_track_task = asyncio.create_task(
        run_slow_track(user_text, emotion_result, user_id, session_id)
    )
    
    routine_result = []
    try:
        # Wait for routine recommendation with a timeout (e.g., 1.0s)
        # If it finishes, we get the result. If not, we proceed without it.
        # This balances "Fast Response" with "Rich Content".
        slow_track_result = await asyncio.wait_for(asyncio.shield(slow_track_task), timeout=1.0)
        routine_result = slow_track_result.get("routine_result", [])
        logger.info(f"ğŸ¢ [Slow Track] Finished within timeout. Items: {len(routine_result)}")
    except asyncio.TimeoutError:
        logger.warning(f"ğŸ¢ [Slow Track] Timed out (continuing in background)")
        # Task continues in background due to asyncio.shield
    except Exception as e:
        logger.error(f"ğŸ¢ [Slow Track] Error: {e}")

    # 4. Context Retrieval (Memory & RAG) - Kept in Fast Track for now for quality
    # Optimization: Could be parallelized with Emotion Analysis if refactored further
    memory_context = ""
    rag_context = ""
    
    try:
        # Memory Layer
        try:
            from .adapters.memory_adapter import get_memories_for_prompt
        except ImportError:
            from adapters.memory_adapter import get_memories_for_prompt
            
        memories = get_memories_for_prompt(session_id, user_id)
        if memories:
            memory_context = f"[ê¸°ì–µëœ ì •ë³´]\n{memories}\n"
            
        # RAG Layer
        try:
            from .conversation_rag_v2 import get_conversation_rag
            rag_store = get_conversation_rag()
            rag_store.add_message(user_id, session_id, "user", user_text)
            similar_msgs = rag_store.search_similar(user_id, user_text, session_id, k=3)
            if similar_msgs:
                rag_context = "[ê³¼ê±° ìœ ì‚¬ ëŒ€í™”]\n"
                for msg in similar_msgs:
                    rag_context += f"- {msg['role']}: {msg['content']} (session: {msg['session_id']})\n"
        except Exception as e:
            logger.error(f"RAG Error: {e}")
            
    except Exception as e:
        logger.error(f"Context Retrieval Error: {e}")
        
    # 5. Generate Response (Fast Track)
    conversation_history = store.get_history(user_id, session_id, limit=None)
    
    ai_response_text = generate_llm_response(
        user_text=user_text,
        emotion_result=emotion_result,
        conversation_history=conversation_history,
        memory_context=memory_context,
        rag_context=rag_context
    )
    
    # 6. Save AI Response
    store.add_message(user_id, session_id, "assistant", ai_response_text)
    
    # Update RAG with AI response
    try:
        if 'rag_store' in locals():
            rag_store.add_message(user_id, session_id, "assistant", ai_response_text)
    except Exception as e:
        logger.error(f"RAG Save Error: {e}")
        
    logger.info(f"âœ… [DeepAgents] Response generated: {ai_response_text[:50]}...")
    
    return {
        "reply_text": ai_response_text,
        "input_text": user_text,
        "emotion_result": emotion_result,
        "routine_result": routine_result, # Now populated if within timeout
        "meta": {
            "model": os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini"),
            "used_tools": ["emotion_analysis"], 
            "session_id": session_id,
            "stt_quality": stt_quality,
            "speaker_id": speaker_id,
            "memory_used": bool(memory_context),
            "rag_used": bool(rag_context),
            "user_id": user_id,
            "storage": "database",
            "api_version": "v2_deepagents"
        }
    }

async def run_ai_bomi_from_audio_v2(
    audio_bytes: bytes,
    user_id: int,
    session_id: str = "default"
) -> dict[str, Any]:
    """
    ìŒì„± ì…ë ¥ ê¸°ë°˜ AI ë´„ì´ ì‹¤í–‰ (DeepAgents Prototype)
    """
    logger.info(f"ğŸ¤ [DeepAgents] Audio processing started (user_id: {user_id})")
    
    # 1. STT
    try:
        from .adapters import run_speech_to_text
    except ImportError:
        from adapters import run_speech_to_text
    
    stt_result = run_speech_to_text(audio_bytes)
    user_text = stt_result["text"]
    stt_quality = stt_result["quality"]
    
    if not user_text:
        return {
            "reply_text": "ì£„ì†¡í•´ìš”, ì˜ ë“¤ë¦¬ì§€ ì•Šì•˜ì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?",
            "input_text": "",
            "emotion_result": None,
            "routine_result": None,
            "meta": {
                "stt_quality": stt_quality,
                "session_id": session_id,
                "user_id": user_id,
                "storage": "database",
                "api_version": "v2_deepagents"
            }
        }
        
    # 2. Delegate to Text Handler
    return await run_ai_bomi_from_text_v2(user_text, user_id, session_id, stt_quality)
