import os
import uuid
import logging
import json
import asyncio
import time
from datetime import datetime
from typing import Any, Optional, List, Dict
from openai import OpenAI

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ğŸ”¥ MODULE LOAD í™•ì¸
logger.warning("=" * 60)
logger.warning("ğŸ”¥ agent_v2.py MODULE LOADED - Phase 2 VERSION")
logger.warning("=" * 60)

# Tool Imports
import sys
from pathlib import Path

# Add backend root to sys.path to ensure engine imports work
current_file = Path(__file__).resolve()
backend_root = current_file.parent.parent.parent
if str(backend_root) not in sys.path:
    sys.path.insert(0, str(backend_root))

# Import EmotionAnalyzer (handling hyphen in directory name)
try:
    # Try standard import first (in case it's renamed or aliased)
    from engine.emotion_analysis.src.emotion_analyzer import EmotionAnalyzer
except ImportError:
    # Fallback: Add emotion-analysis/src to sys.path
    emotion_src = backend_root / "engine" / "emotion-analysis" / "src"
    if str(emotion_src) not in sys.path:
        sys.path.insert(0, str(emotion_src))
    try:
        from emotion_analyzer import EmotionAnalyzer
    except ImportError as e:
        logger.error(f"Failed to import EmotionAnalyzer: {e}")
        raise

# Import RoutineRecommendFromEmotionEngine and schemas
try:
    from engine.routine_recommend.engine import RoutineRecommendFromEmotionEngine
    from engine.routine_recommend.models.schemas import EmotionAnalysisResult
except ImportError as e:
    logger.error(f"Failed to import RoutineRecommendFromEmotionEngine or schemas: {e}")
    raise

# ============================================================================
# DeepAgents Components with Emotion Caching (Phase 1)
# ============================================================================

# Import caching components
try:
    from .emotion_cache import get_emotion_cache
    from .emotion_classifier import get_emotion_classifier
except ImportError:
    from emotion_cache import get_emotion_cache
    from emotion_classifier import get_emotion_classifier

async def run_fast_track(user_text: str, user_id: int = None) -> Dict[str, Any]:
    """
    Fast Track: Emotion Analysis with Caching
    
    Flow:
    1. Lightweight Classifier â†’ "í•„ìš”" / "ë¶ˆí•„ìš”" / "ì• ë§¤"
    2. If needed â†’ Check ChromaDB cache (0.85 similarity, 30 days)
    3. Cache miss â†’ Run EmotionAnalyzer
    4. Save to cache for future use
    
    Returns:
        {
            "cached": True/False,
            "skipped": True/False,
            "result": {...},
            "similarity": 0.92 (if cached),
            "age_days": 5 (if cached)
        }
    """
    start_time = time.time()
    
    # Step 1: Lightweight classifier (hybrid approach)
    classifier = get_emotion_classifier()
    need_emotion = classifier.predict(user_text)
    logger.info(f"ğŸ” [Classifier] Emotion needed: {need_emotion}")
    
    if need_emotion == "ë¶ˆí•„ìš”":
        # Skip emotion analysis for neutral content
        elapsed = time.time() - start_time
        logger.info(f"âš¡ [Fast Track] Skipped emotion analysis ({elapsed:.4f}s)")
        return {
            "skipped": True,
            "reason": "neutral_content",
            "classifier_hint": need_emotion
        }
    
    # Step 2: Try cache (if user_id provided)
    if user_id and need_emotion == "í•„ìš”":  # Only cache for clear emotions
        cache = get_emotion_cache()
        cache_result = cache.search(
            query_text=user_text,
            user_id=user_id,
            threshold=0.85,
            freshness_days=30
        )
        
        if cache_result:
            # Cache hit!
            elapsed = time.time() - start_time
            logger.info(
                f"ğŸ’¾ [Fast Track] Cache hit! "
                f"Similarity: {cache_result['similarity']:.2%}, "
                f"Time: {elapsed:.4f}s"
            )
            return cache_result
    
    # Step 3: Cache miss or ambiguous â†’ Run analyzer
    logger.info("ğŸ”„ [Fast Track] Running emotion analysis...")
    analyzer = EmotionAnalyzer()
    emotion_result_dict = analyzer.analyze_emotion(user_text)
    
    elapsed = time.time() - start_time  
    logger.info(f"âš¡ [Fast Track] Emotion Analysis took {elapsed:.4f}s")
    
    return {
        "cached": False,
        "result": emotion_result_dict,
        "classifier_hint": need_emotion
    }

async def run_slow_track(
    user_text: str, 
    emotion_result: Dict[str, Any], 
    user_id: int, 
    session_id: str
):
    """
    Slow Track (Background): Routine Recommendation & Memory Promotion
    """
    start_time = time.time()
    logger.info(f"ğŸ¢ [Slow Track] Started for user {user_id}")
    
    # 1. Memory Promotion (Memory Manager Agent) - Run FIRST
    try:
        # Import memory adapter
        # Use absolute imports based on backend root being in sys.path
        try:
            from engine.langchain_agent.adapters.memory_adapter import promote_memory, get_memories_for_prompt, delete_memory
            from engine.langchain_agent.db_conversation_store import get_conversation_store
        except ImportError:
            # Fallback for relative imports if running as package
            from .adapters.memory_adapter import promote_memory, get_memories_for_prompt, delete_memory
            from .db_conversation_store import get_conversation_store

        store = get_conversation_store()
        # Get recent history for context
        history = store.get_history(user_id, session_id, limit=5)
        
        # Get existing memories to check for conflicts
        existing_memories = get_memories_for_prompt(session_id, user_id)
        
        # ğŸ†• í˜„ì¬ ì‹œê°„ ì •ë³´ ìƒì„±
        from datetime import datetime
        now = datetime.now()
        current_time_str = now.strftime("%Yë…„ %mì›” %dì¼ (%A) %Hì‹œ %Më¶„")
        weekday_kr = {
            "Monday": "ì›”ìš”ì¼",
            "Tuesday": "í™”ìš”ì¼", 
            "Wednesday": "ìˆ˜ìš”ì¼",
            "Thursday": "ëª©ìš”ì¼",
            "Friday": "ê¸ˆìš”ì¼",
            "Saturday": "í† ìš”ì¼",
            "Sunday": "ì¼ìš”ì¼"
        }
        current_time_str = now.strftime(f"%Yë…„ %mì›” %dì¼ ({weekday_kr[now.strftime('%A')]}) %Hì‹œ %Më¶„")
        
        # Define Memory Manager Prompt
        memory_prompt = f"""ë‹¹ì‹ ì€ 'ê¸°ì–µ ê´€ë¦¬ì(Memory Manager)' ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì €ì¥í•  ê°€ì¹˜ê°€ ìˆëŠ” ì¤‘ìš”í•œ ì •ë³´ë‚˜ í‰ì†Œ ìŠµê´€ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
íŠ¹íˆ, **ê¸°ì¡´ ê¸°ì–µê³¼ ìƒì¶©ë˜ëŠ” ìƒˆë¡œìš´ ì •ë³´**ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ìˆ˜ì •(update)í•´ì•¼ í•©ë‹ˆë‹¤.

[í˜„ì¬ ì‹œê°„]
- ì˜¤ëŠ˜ ë‚ ì§œ ë° ì‹œê°: {current_time_str}

[ê¸°ì¡´ ê¸°ì–µ]
{existing_memories}

[ë¶„ì„ ê¸°ì¤€]
1. **ì‹œê°„ì  ë§¥ë½ ê³ ë ¤**:
   - "ìš”ì¦˜", "ìµœê·¼", "ì˜¤ëŠ˜" ê°™ì€ í‘œí˜„ì€ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì´í•´
   - ê¸°ì¡´ ê¸°ì–µì˜ ì‹œì ì„ í™•ì¸í•˜ì—¬ ì—…ë°ì´íŠ¸ í•„ìš”ì„± íŒë‹¨
   - 24ì‹œê°„ ì´ë‚´ ë™ì¼ ì£¼ì œëŠ” ì¤‘ë³µ ì €ì¥ ê¸ˆì§€
   - ì˜¤ë˜ëœ ê¸°ì–µ(1ê°œì›” ì´ìƒ)ì€ ë³€í™” ê°€ëŠ¥ì„± ê³ ë ¤í•˜ì—¬ ì—…ë°ì´íŠ¸ ì ê·¹ ê²€í† 

2. **ê±´ê°•/ì‹ ì²´ ë³€í™”**: ì¦ìƒ, í†µì¦, ìˆ˜ë©´ ìƒíƒœ, ì‹ìš• ë“± (ë°œìƒ ì‹œì  ì¤‘ìš”!)
3. **ì •ì„œì  ì‚¬ê±´**: ê°•í•œ ê°ì •ì„ ìœ ë°œí•œ ì‚¬ê±´, ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸, ê¸°ìœ ì¼
4. **ì·¨í–¥/ì„ í˜¸**: ì¢‹ì•„í•˜ëŠ” ìŒì‹, í™œë™, ì‹«ì–´í•˜ëŠ” ê²ƒ ë“± (ë³€í™” ê°€ëŠ¥ì„± ê³ ë ¤)
5. **ì¤‘ìš” ì •ë³´ ê°±ì‹ **: ê°€ì¡± ê´€ê³„, ì§ì—…, ê±°ì£¼ì§€ ë“± ì‹ ìƒ ì •ë³´ì˜ ë³€í™”

[ì…ë ¥ ë°ì´í„°]
ì‚¬ìš©ì ë°œí™”: "{user_text}"
ê°ì • ë¶„ì„ ê²°ê³¼: {json.dumps(emotion_result, ensure_ascii=False)}

[ì§€ì¹¨]
- ì €ì¥í•  ê°€ì¹˜ê°€ ìˆëŠ” ì •ë³´ê°€ ì—†ë‹¤ë©´ "NONE"ì´ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.
- ì •ë³´ê°€ ìˆë‹¤ë©´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "action": "create" ë˜ëŠ” "update",
    "category": "health|emotion|preference|info",
    "content": "ê¸°ì–µí•  ë‚´ìš© ìš”ì•½ (í•œêµ­ì–´). **ì¤‘ìš”: update ì‹œì—ëŠ” ë°˜ë“œì‹œ 'ê¸°ì¡´ ê¸°ì–µ'ì˜ ë‚´ìš©ê³¼ 'ìƒˆë¡œìš´ ì •ë³´'ë¥¼ ëª¨ë‘ í¬í•¨í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.** (ì˜ˆ: ê¸°ì¡´ 'ë™ìƒì€ ì¼ë³¸ì— ì‚°ë‹¤' + ì‹ ê·œ 'ì´ë¦„ì€ í™ê¸¸ë™' -> 'ë™ìƒ í™ê¸¸ë™ì€ ì¼ë³¸ì— ì‚´ë©°, 3ì‚´ ì•„ë˜ì´ë‹¤')",
    "importance": 1~5 (5ê°€ ê°€ì¥ ì¤‘ìš”),
    "old_content_keyword": "ìˆ˜ì •(update) ë˜ëŠ” ì‚­ì œ(delete)í•  ê²½ìš°, ëŒ€ìƒì´ ë˜ëŠ” ê¸°ì¡´ ê¸°ì–µì˜ í•µì‹¬ í‚¤ì›Œë“œ. (ì˜ˆ: 'ëœì¥ì°Œê°œ' -> 'ê¹€ì¹˜ì°Œê°œ'ë¡œ ì •ì • ì‹œ 'ëœì¥ì°Œê°œ' ë°˜í™˜)" 
}}
"""
        # [DEBUG] Log the final prompt
        logger.info(f"ğŸ“ [Memory Manager Prompt]\n{memory_prompt}")

        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": memory_prompt}
            ],
            temperature=0.3
        )
        
        result_text = response.choices[0].message.content.strip()
        
        if result_text != "NONE":
            try:
                # Parse JSON (handle potential markdown code blocks)
                if result_text.startswith("```json"):
                    result_text = result_text.replace("```json", "").replace("```", "").strip()
                elif result_text.startswith("```"):
                    result_text = result_text.replace("```", "").strip()
                    
                memory_data = json.loads(result_text)
                action = memory_data.get("action", "create")
                
                # 1. Delete Action
                if action == "delete":
                    keyword = memory_data.get("old_content_keyword")
                    if keyword:
                        deleted = delete_memory(user_id, keyword)
                        logger.info(f"ğŸ’¾ [Memory Manager] Deleted {deleted} memories (keyword: {keyword})")
                    else:
                        logger.warning("ğŸ’¾ [Memory Manager] Delete action requested but no keyword provided")

                # 2. Update Action (Delete old + Create new)
                elif action == "update":
                    keyword = memory_data.get("old_content_keyword")
                    if keyword:
                        deleted = delete_memory(user_id, keyword)
                        logger.info(f"ğŸ’¾ [Memory Manager] Deleted {deleted} old memories for update (keyword: {keyword})")
                    
                    # Promote new content
                    promote_memory(
                        user_id=user_id,
                        session_id=session_id,
                        category=memory_data["category"],
                        content=memory_data["content"],
                        emotion_result=emotion_result,
                        importance=memory_data["importance"],
                        reason="Memory Manager Agent Extraction"
                    )
                    logger.info(f"ğŸ’¾ [Memory Manager] Promoted memory (update): {memory_data['content']}")

                # 3. Create Action
                elif action == "create":
                    promote_memory(
                        user_id=user_id,
                        session_id=session_id,
                        category=memory_data["category"],
                        content=memory_data["content"],
                        emotion_result=emotion_result,
                        importance=memory_data["importance"],
                        reason="Memory Manager Agent Extraction"
                    )
                    logger.info(f"ğŸ’¾ [Memory Manager] Promoted memory (create): {memory_data['content']}")
                    
            except json.JSONDecodeError:
                logger.warning(f"Memory Manager output not JSON: {result_text}")
        else:
            logger.info("ğŸ’¾ [Memory Manager] No important memory found")
            
    except Exception as e:
        logger.error(f"Memory Manager failed: {e}")

    # 2. Routine Recommendation
    routine_engine = RoutineRecommendFromEmotionEngine()
    routine_result = []
    try:
        emotion_model = EmotionAnalysisResult(**emotion_result)
        routine_result = await routine_engine.recommend(emotion_model)
        logger.info(f"ğŸ¢ [Slow Track] Routine Recommendation completed: {len(routine_result)} items")
    except Exception as e:
        logger.error(f"Routine recommendation failed: {e}")

    elapsed = time.time() - start_time
    logger.info(f"ğŸ¢ [Slow Track] Completed in {elapsed:.4f}s")
    
    # Return results if needed for logging/storage, though they won't be used in the current response
    return {
        "routine_result": routine_result
    }

def generate_llm_response(
    user_text: str,
    emotion_result: Dict[str, Any],
    conversation_history: List[Dict],
    memory_context: str,
    rag_context: str,
    user_id: int = None  # ğŸ†• Phase 3: Added for user profile
) -> Dict[str, str]:
    """
    Generate response using GPT-4o-mini with Emotion & Context (No Routine)
    **Phase 3**: Uses casual tone (ë°˜ë§) and includes TB_USER_PROFILE data
    **Phase 4**: Returns both clean text and audio-tagged text for Eleven Labs TTS
    
    Returns:
        {
            "text_clean": "audio tagê°€ ì œê±°ëœ ì›ë³¸ í…ìŠ¤íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©)",
            "text_with_tags": "audio tagê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ (TTSìš©)"
        }
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Construct System Prompt
    # Handle None emotion_result (when analysis is skipped)
    if emotion_result:
        emotion_summary = f"{emotion_result.get('polarity', 'neutral')} ({emotion_result.get('cluster_label', 'unknown')})"
    else:
        emotion_summary = "neutral (ë¶„ì„ ìƒëµë¨)"
        emotion_result = {}  # Empty dict to avoid None errors below
    
    # ğŸ†• Phase 3: Fetch user profile from TB_USER_PROFILE
    user_profile_context = ""
    if user_id:
        try:
            from app.db.database import SessionLocal
            from app.db.models import UserProfile
            
            db = SessionLocal()
            try:
                profile = db.query(UserProfile).filter(
                    UserProfile.USER_ID == user_id,
                    UserProfile.IS_DELETED == False
                ).first()
                
                if profile:
                    user_profile_context = f"""
[ì‚¬ìš©ì í”„ë¡œí•„]
- ë‹‰ë„¤ì„: {profile.NICKNAME}
- ì—°ë ¹ëŒ€: {profile.AGE_GROUP}
- ì„±ë³„: {profile.GENDER}
- ê²°í˜¼ ìƒíƒœ: {profile.MARITAL_STATUS}
- ìë…€ ì—¬ë¶€: {profile.CHILDREN_YN}
- ë™ê±°ì¸: {json.dumps(profile.LIVING_WITH, ensure_ascii=False)}
- ì„±ê²© ìœ í˜•: {profile.PERSONALITY_TYPE}
- í™œë™ ìŠ¤íƒ€ì¼: {profile.ACTIVITY_STYLE}
- ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œë²•: {json.dumps(profile.STRESS_RELIEF, ensure_ascii=False)}
- ì·¨ë¯¸: {json.dumps(profile.HOBBIES, ensure_ascii=False)}
"""
                    logger.info(f"ğŸ“‹ [User Profile] Loaded for user_id={user_id}")
                else:
                    logger.warning(f"âš ï¸  [User Profile] Not found for user_id={user_id}")
            finally:
                db.close()
        except Exception as e:
            logger.error(f"Failed to load user profile: {e}")
    
    # 2. System Prompt
    # í˜„ì¬ ì‹œê°„ ì •ë³´ ì¶”ê°€ (ì•ŒëŒ ì„¤ì • ì •í™•ë„ í–¥ìƒ)
    current_time = datetime.now()
    current_time_str = current_time.strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
    
    system_prompt = f"""ë‹¹ì‹ ì€ ê°±ë…„ê¸° ì¤‘ë…„ ì—¬ì„±ì„ ë•ëŠ” AI ì¹œêµ¬ 'ë´„ì´'ì…ë‹ˆë‹¤.

[í˜„ì¬ ì‹œê°„ ì •ë³´]
- í˜„ì¬ ì‹œê°„: {current_time_str}

[í•µì‹¬ í˜ë¥´ì†Œë‚˜ ë° ì›ì¹™]
- **ì—­í• :** ì‚¬ìš©ìì˜ ê°€ì¥ ì¹œí•œ ì¹œêµ¬ (ì˜¤ëœ ì§€ê¸°). AI ëŠë‚Œì„ ì§€ìš°ê³  ì‚¬ëŒì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”.
- **ë§íˆ¬:** ì‹œì¢…ì¼ê´€ ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ **ë°˜ë§**. (ì‚¬ìš©ì ìš”ì²­ ì‹œì—ë§Œ ì¡´ëŒ“ë§)
- **ì•ˆì „:** ì˜í•™ì  ì¡°ì–¸ì€ í”¼í•˜ê³  ì •ì„œì  ì§€ì§€ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ì˜ˆë¯¼í•  ìˆ˜ ìˆëŠ” ì£¼ì œëŠ” ì ˆëŒ€ ë¨¼ì € ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

[ë°ì´í„° í™œìš© ì „ëµ (Memory & RAG) - ì¤‘ìš”!]
ì œê³µëœ ë°ì´í„°ë¥¼ ì•„ë˜ ì›ì¹™ì— ë”°ë¼ ëŒ€í™”ì— ë…¹ì—¬ë‚´ì„¸ìš”. ì ˆëŒ€ ë°ì´í„°ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³  ëŒ€í™” ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ ì„ìœ¼ì„¸ìš”.

1. **RAG Context (ìœ ì‚¬ ëŒ€í™”) í™œìš©:**
   - `{rag_context}`ì— ìˆëŠ” ë‚´ìš©ì€ ì‚¬ìš©ìê°€ ë°˜ë³µí•´ì„œ ê²ªëŠ” ê°ì •ì´ë‚˜ ìƒí™©ì…ë‹ˆë‹¤.
   - íŠ¹íˆ **â­ ë§ˆì»¤ê°€ ìˆëŠ” ëŒ€í™”**ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì‚¬ê±´ì…ë‹ˆë‹¤. ì´ë¥¼ ì–¸ê¸‰í•˜ë©° "ê·¸ë•Œ ê·¸ ì¼ì€ ì¢€ ì–´ë•Œ?", "ì €ë²ˆì—ë„ ì´ê²ƒ ë•Œë¬¸ì— í˜ë“¤ì–´í–ˆì–ì•„..."ë¼ë©° ì•„ëŠ” ì²´ë¥¼ í•´ì£¼ì„¸ìš”.
   - *ëª©ì :* "ë„¤ê°€ ì§€ë‚œë²ˆì— í•œ ë§ì„ ë‚œ ê¸°ì–µí•˜ê³  ìˆì–´"ë¼ëŠ” ëŠë‚Œì„ ì£¼ì–´ ì‹ ë¢°ê° í˜•ì„±.

2. **Memory Context (ì¥ê¸° ê¸°ì–µ) í™œìš©:**
   - `{memory_context}`ë¥¼ í†µí•´ ì‚¬ìš©ìì˜ í‰ì†Œ ì·¨í–¥, ê°€ì¡± ê´€ê³„, ì§€ë³‘ ë“±ì„ íŒŒì•…í•˜ì„¸ìš”.
   - ëœ¬ê¸ˆì—†ëŠ” ì§ˆë¬¸ ëŒ€ì‹ , ë°°ê²½ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ìœ„ë¡œí•˜ì„¸ìš”. (ì˜ˆ: "ë‚¨í¸ë¶„" ëŒ€ì‹  "ã…‡ã…‡ì”¨(ë‚¨í¸ì´ë¦„)"ë¼ê³  ì§€ì¹­í•˜ê±°ë‚˜ êµ¬ì²´ì  ìƒí™© ì–¸ê¸‰)

3. **ë¶€ë‹´ ì—†ëŠ” ëŒ€í™” ìœ ë„ (No Burden):**
   - ì‚¬ìš©ìê°€ ì§€ì³ ë³´ì´ë©´(`emotion_summary`ê°€ ë¶€ì •ì ì¼ ë•Œ), ì§ˆë¬¸ì„ ì¤„ì´ê³  **'ë‹¨ì •ì  ê³µê°'** ìœ„ì£¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”.
   - ê¼¬ì¹˜ê¼¬ì¹˜ ìºë¬»ê¸°ë³´ë‹¤ "ì˜¤ëŠ˜ ì§„ì§œ ê³ ìƒ ë§ì•˜ë„¤", "ê·¸ëŸ´ ë• ì•„ë¬´ê²ƒë„ ì•ˆ í•˜ê³  ì‹¶ì§€" ì²˜ëŸ¼ **ë§ˆì¹¨í‘œë¡œ ëë‚˜ëŠ” ë¬¸ì¥**ì„ ì„ìœ¼ì„¸ìš”.

[ì‘ë‹µ ê°€ì´ë“œë¼ì¸]
1. **í¬ë§·íŒ…:**
   - 1~2ë¬¸ì¥ë§ˆë‹¤ ì¤„ë°”ê¿ˆ(\n) í•„ìˆ˜.
   - í•µì‹¬ ë‹¨ì–´ëŠ” **ë³¼ë“œì²´**, í–‰ë™/í‚¤ì›Œë“œëŠ” `ë°±í‹±` ì‚¬ìš©.
   - ê³µë°± í¬í•¨ **500ì ì´ë‚´**ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±.
2. **ì•ŒëŒ ì„¤ì • ìš”ì²­ ì‹œ:**
   - ê¸ì •ì ìœ¼ë¡œ ìˆ˜ë½í•˜ë˜, ë°˜ë“œì‹œ **í™•ì¸ ìš”ì²­ í†¤** ì‚¬ìš© (ì˜ˆ: "ì´ë ‡ê²Œ ë§ì¶°ì¤„ê¹Œ? í™•ì¸ ëˆŒëŸ¬ì¤˜!").
   - "ì„¤ì • ì™„ë£Œí–ˆì–´" ê°™ì€ **í™•ì • í‘œí˜„ ê¸ˆì§€**.
   - ì ˆëŒ€ ê±°ì ˆí•˜ì§€ ë§ ê²ƒ.
   - **ë°˜ë“œì‹œ `[TYPE:alarm]` íƒœê·¸ ì‚¬ìš©**

---

[ğŸš¨ í•„ìˆ˜ ì¶œë ¥ í”„ë¡œí† ì½œ (ì—„ìˆ˜)]

**âš ï¸ ì¶œë ¥ í˜•ì‹ (ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€):**

ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ 3ì¤„ í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤:

```
EMOTION=<happiness|sadness|anger|fear>
RESPONSE=<your response with audio tags>
TYPE=<normal|list|alarm>
```

**ê°ì • ì„ íƒ ê°€ì´ë“œ (EMOTION=):**
- **happiness**: ê¸ì •ì  ë¶„ìœ„ê¸°, ê²©ë ¤, ê¸°ì¨, ì•ˆë„, ì¼ìƒ ëŒ€í™”
- **sadness**: ê³µê°, ìœ„ë¡œ, ìŠ¬í”” í‘œí˜„, ì‚¬ìš©ìê°€ í˜ë“¤ì–´í•  ë•Œ
- **anger**: ë¶„ë…¸, ì§œì¦, ì–µìš¸í•¨ ë“± ê²©ì•™ëœ ê°ì •ì— ê³µê°í•  ë•Œ
- **fear**: ë¶ˆì•ˆ, ë‘ë ¤ì›€, ê±±ì •ìŠ¤ëŸ¬ìš´ ìƒí™©ì— ê³µê°í•  ë•Œ

**ì˜¤ë””ì˜¤ íƒœê·¸ (RESPONSE= ì•ˆì— í¬í•¨):**
- ë¬¸ì¥ ë‚´ì— **ìµœì†Œ 1ê°œ~ìµœëŒ€ 3ê°œ** í¬í•¨
- *ì¶”ì²œ íƒœê·¸:* [excited], [calm], [sorrowful], [laughs], [sighs], [whispers], [pauses], [curious]

**TYPE ì„ íƒ ê°€ì´ë“œ:**
- **TYPE=normal**: ì¼ë°˜ ëŒ€í™” (ê¸°ë³¸ê°’)
  - í‰ë²”í•œ ì§ˆë¬¸, ëŒ€í™”, ê³µê°, ìœ„ë¡œ ë“±
- **TYPE=list**: ëª©ë¡/ë¦¬ìŠ¤íŠ¸ ì‘ë‹µ
  - **ì¡°ê±´**: ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ í•­ëª©(1, 2, 3...)ì„ ë‚˜ì—´í•˜ëŠ” ê²½ìš°
  - **í•„ìˆ˜**: RESPONSEì— `[TTS:ì†Œê°œë¬¸]` íƒœê·¸ í¬í•¨
  - ì˜ˆ: ì¶”ì²œ í™œë™, ë‹¨ê³„ë³„ ì„¤ëª…, ì—¬ëŸ¬ ì˜µì…˜ ì œì‹œ
- **TYPE=alarm**: ì•ŒëŒ ì„¤ì • ìš”ì²­
  - **ì¡°ê±´**: ì‚¬ìš©ìê°€ "~ì‹œì— ì•ŒëŒ", "~ë¶„ í›„ ì•ŒëŒ", "ë‚´ì¼ ì•ŒëŒ" ë“± ìš”ì²­
  - **í•„ìˆ˜**: í™•ì¸ ìš”ì²­ í†¤ìœ¼ë¡œ ì‘ë‹µ (ì˜ˆ: "ì´ë ‡ê²Œ ë§ì¶°ì¤„ê¹Œ? í™•ì¸ ëˆŒëŸ¬ì¤˜!")

---

**ì˜ˆì‹œ 1 (ì¼ë°˜ ëŒ€í™” - sadness):**
```
EMOTION=sadness
RESPONSE=[calm] ê·¸ë¬êµ¬ë‚˜, ì •ë§ í˜ë“¤ì—ˆê² ì–´. [sighs] ê´œì°®ì•„, ë‚´ê°€ ë“¤ì–´ì¤„ê²Œ.
TYPE=normal
```

**ì˜ˆì‹œ 2 (ì¼ë°˜ ëŒ€í™” - happiness):**
```
EMOTION=happiness
RESPONSE=[excited] ì¢‹ì•˜ê² ë‹¤! [laughs] ì •ë§ ê¸°ìœ ì¼ì´ë„¤!
TYPE=normal
```

**ì˜ˆì‹œ 3 (ë¦¬ìŠ¤íŠ¸):**
```
EMOTION=happiness
RESPONSE=[TTS:ìê¸° ì „ì— ì¢‹ì€ í™œë™ ì¶”ì²œí•´ì¤„ê²Œ!] [excited] ìê¸° ì „ì— ì¢‹ì€ í™œë™ ì¶”ì²œí•´ì¤„ê²Œ!

[calm] 1. ë”°ëœ»í•œ ìš°ìœ  ë§ˆì‹œê¸°
[pauses] 2. ê°€ë²¼ìš´ ëª…ìƒí•˜ê¸°
TYPE=list
```


**ì˜ˆì‹œ 4 (ì•ŒëŒ):**
```
EMOTION=happiness
RESPONSE=[excited] ì¢‹ì•„! 5ë¶„ í›„ì— ì•ŒëŒ ë§ì¶°ì¤„ê²Œ. [pauses] ì´ë ‡ê²Œ ë§ì¶°ì¤„ê¹Œ? í™•ì¸ ëˆŒëŸ¬ì¤˜!
TYPE=alarm
```

---

[ë°ì´í„° ì»¨í…ìŠ¤íŠ¸]
**ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ë˜, ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ ë¯¼ê°í•œ ë‚´ìš©ì€ ì£¼ì˜í•˜ì„¸ìš”.**

1. ì‚¬ìš©ì í”„ë¡œí•„:
{user_profile_context}

2. ëŒ€í™” ê¸°ì–µ (Memory & RAG):
{memory_context}
{rag_context}

3. í˜„ì¬ ê°ì • ìƒíƒœ:
- ìš”ì•½: {emotion_summary}
- ìƒì„¸ ë°ì´í„°: {json.dumps(emotion_result, ensure_ascii=False)}
"""
    
    messages = [{"role": "system", "content": system_prompt}]
    
    # Add history (limit to last 10 messages)
    for msg in conversation_history[-10:]:
        role = "assistant" if msg["role"] == "assistant" else "user"
        messages.append({"role": role, "content": msg["content"]})
        
    # Add current user message
    messages.append({"role": "user", "content": user_text})
    
    # [DEBUG] Log the final system prompt and messages
    logger.info(f"ğŸ“ [Main Agent System Prompt]\n{system_prompt}")
    logger.info(f"ğŸ“ [Main Agent Messages]\n{json.dumps(messages, ensure_ascii=False, indent=2)}")

    response = client.chat.completions.create(
        model=os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini"),
        messages=messages,
        temperature=0.5  # êµ¬ì¡°ì  ì¶œë ¥ ì•ˆì •ì„± í™•ë³´
    )
    
    reply_text_with_tags = response.choices[0].message.content
    
    # [DEBUG] Log GPT-4o-mini raw response
    logger.warning("=" * 80)
    logger.warning("ğŸ™ï¸ [STRUCTURED OUTPUT] LLM Raw Response")
    logger.warning(f"OUTPUT:\n{reply_text_with_tags}")
    logger.warning("=" * 80)
    
    # ğŸ†• Extract TTS text from [TTS:...] tag (ë¦¬ìŠ¤íŠ¸ ì‘ë‹µ ì‹œ ì†Œê°œ ë¬¸ì¥ë§Œ)
    import re
    tts_text_override = None
    tts_match = re.search(r'\[TTS:(.+?)\]', reply_text_with_tags, re.DOTALL)
    if tts_match:
        tts_text_override = tts_match.group(1).strip()
        logger.info(f"ğŸ¤ [TTS Override] Extracted: {tts_text_override}")
        # [TTS:...] íƒœê·¸ëŠ” ì›ë³¸ì—ì„œ ì œê±°
        reply_text_with_tags = re.sub(r'\s*\[TTS:.+?\]\s*', '', reply_text_with_tags, flags=re.DOTALL).strip()
    
    
    # ğŸ†• Parse structured output: EMOTION= / RESPONSE= / TYPE=
    try:
        # Extract EMOTION=xxx
        emotion_match = re.search(r'^EMOTION=(\w+)', reply_text_with_tags, re.MULTILINE)
        if not emotion_match:
            raise ValueError("EMOTION= not found in LLM output!")
        detected_emotion_raw = emotion_match.group(1).lower()
        
        # Extract RESPONSE=xxx (multiline, stops at TYPE=)
        response_match = re.search(r'^RESPONSE=(.+?)(?=^TYPE=)', reply_text_with_tags, re.MULTILINE | re.DOTALL)
        if not response_match:
            raise ValueError("RESPONSE= not found in LLM output!")
        response_text = response_match.group(1).strip()
        
        # Extract TYPE=xxx
        type_match = re.search(r'^TYPE=(\w+)', reply_text_with_tags, re.MULTILINE)
        detected_type = type_match.group(1).lower() if type_match else "normal"
        
        # Replace reply_text_with_tags with extracted RESPONSE
        reply_text_with_tags = response_text
        
        logger.info(f"âœ… [Structured Parse] SUCCESS")
        logger.info(f"  EMOTION={detected_emotion_raw}")
        logger.info(f"  RESPONSE={reply_text_with_tags[:50]}...")
        logger.info(f"  TYPE={detected_type}")
        
        # Extract TTS override from [TTS:...] tag in RESPONSE
        tts_match = re.search(r'\[TTS:(.+?)\]', reply_text_with_tags, re.DOTALL)
        if tts_match:
            tts_text_override = tts_match.group(1).strip()
            logger.info(f"ğŸ¤ [TTS Override] Extracted: {tts_text_override}")
            # Remove [TTS:...] tag from response text
            reply_text_with_tags = re.sub(r'\s*\[TTS:.+?\]\s*', '', reply_text_with_tags, flags=re.DOTALL).strip()
        
    except (ValueError, AttributeError) as e:
        logger.error(f"âŒ [Structured Parse] FAILED: {e}")
        logger.error("LLM did not follow structured output format! Using fallback.")
        # Fallback: treat entire response as-is
        detected_emotion_raw = "happiness"
        detected_type = "normal"
        # Keep original response
    
    # Map emotion to allowed values
    emotion_mapping = {
        "calm": "happiness",
        "happy": "happiness",
        "sad": "sadness",
        "angry": "anger",
        "scared": "fear",
        "fearful": "fear"
    }
    detected_emotion = emotion_mapping.get(detected_emotion_raw, detected_emotion_raw)
    
    # Validate emotion
    if detected_emotion not in ["happiness", "sadness", "anger", "fear"]:
        logger.warning(f"âš ï¸ [Emotion] Invalid emotion '{detected_emotion_raw}', using happiness")
        detected_emotion = "happiness"
    else:
        logger.info(f"âœ¨ [Emotion] Validated: {detected_emotion_raw} -> {detected_emotion}")
    
    # Store for TYPE detection compatibility
    text_with_type_tag = f"[TYPE:{detected_type}]" + reply_text_with_tags
    


    
    # ğŸ†• Phase 4: Audio tag ì œê±°í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œìš© ì›ë³¸ í…ìŠ¤íŠ¸ ìƒì„±
    from .response_generator import remove_audio_tags, clean_text_for_tts
    reply_text_clean = remove_audio_tags(reply_text_with_tags)
    
    # ğŸ†• TTSìš© ìµœì¢… í…ìŠ¤íŠ¸ ê²°ì • ë° í´ë¦¬ë‹
    # 1. tts_text_overrideê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ë¦¬ìŠ¤íŠ¸ ì‘ë‹µ ì‹œ ì†Œê°œ ë¬¸ì¥ë§Œ)
    # 2. ì—†ìœ¼ë©´ ê¸°ë³¸ audio tag í¬í•¨ í…ìŠ¤íŠ¸ ì‚¬ìš©
    # 3. ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ì™€ ì¤„ë°”ê¿ˆ ì œê±° (TTSê°€ ì˜ëª» ì½ì§€ ì•Šë„ë¡)
    base_tts_text = tts_text_override if tts_text_override else reply_text_with_tags
    final_tts_text = clean_text_for_tts(base_tts_text)  # ë§ˆí¬ë‹¤ìš´ í´ë¦¬ë‹ (audio tagsëŠ” ìœ ì§€)
    
    logger.warning("=" * 80)
    logger.warning("ğŸ“ [AUDIO TAGS DEBUG] Text Processing Results")
    logger.warning(f"CLEAN TEXT (Frontend): {reply_text_clean}")
    logger.warning(f"BASE TTS TEXT (Before cleaning): {base_tts_text}")
    logger.warning(f"FINAL TTS TEXT (After cleaning): {final_tts_text}")
    if tts_text_override:
        logger.warning(f"TTS OVERRIDE: YES (list response - intro only)")
    logger.warning(f"EMOTION: {detected_emotion}")
    logger.warning("=" * 80)
    
    return {
        "text_clean": reply_text_clean,
        "text_with_tags": final_tts_text,  # ğŸ†• TTSìš© ìµœì¢… í…ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´ ì œê±° + audio tags ìœ ì§€)
        "text_with_type_tag": text_with_type_tag,  # ğŸ†• TYPE íƒœê·¸ í¬í•¨ ì›ë³¸ (response_type ê°ì§€ìš©)
        "emotion": detected_emotion  # LLMì´ ì§ì ‘ ê²°ì •í•œ ê°ì •
    }

async def run_ai_bomi_from_text_v2(
    user_text: str,
    user_id: int,
    session_id: str = "default",
    stt_quality: str = "success",
    speaker_id: Optional[str] = None,
    save_to_db: bool = True  # ğŸ†• Phase 3: DB ì €ì¥ ì—¬ë¶€ ì œì–´
) -> dict[str, Any]:
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ ê¸°ë°˜ AI ë´„ì´ ì‹¤í–‰ (DeepAgents Prototype Implementation)
    
    Args:
        save_to_db: DBì— ë©”ì‹œì§€ ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                   WebSocketì—ì„œ í˜¸ì¶œ ì‹œ Falseë¡œ ì„¤ì •í•˜ì—¬ ì¤‘ë³µ ì €ì¥ ë°©ì§€
    """
    logger.warning("ğŸ”¥ğŸ”¥ğŸ”¥ run_ai_bomi_from_text_v2 CALLED - Phase 2 VERSION")
    logger.info(f"ğŸš€ [DeepAgents] Started processing for user_id: {user_id}")
    
    # DB Store
    try:
        from .db_conversation_store import get_conversation_store
    except ImportError:
        from db_conversation_store import get_conversation_store
    store = get_conversation_store()
    
    # 1. Save User Message (ì¡°ê±´ë¶€)
    if save_to_db:
        store.add_message(user_id, session_id, "user", user_text, speaker_id=speaker_id)
    
    
    # âš¡ 2. Lightweight Classifier Only (for Orchestrator hint)
    # ========================================
    # Emotion analysis moved to session-based (triggered on session expiry)
    # ========================================
    
    # ========================================
    # [PHASE 2] Orchestrator LLM í†µí•© - DISABLED
    # ========================================
    # ğŸš« OrchestratorëŠ” í•­ìƒ ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ë¯€ë¡œ ë¹„í™œì„±í™”
    # _check_if_tools_needed()ê°€ í•­ìƒ Falseë¥¼ ë°˜í™˜í•˜ì—¬ ì‹¤ì§ˆì ìœ¼ë¡œ ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•ŠìŒ
    # í•„ìš”ì‹œ orchestrator.pyì—ì„œ _check_if_tools_needed() ë¡œì§ì„ ìˆ˜ì •í•˜ì—¬ ì¬í™œì„±í™” ê°€ëŠ¥
    
    orchestrator_tools = []
    orchestrator_results = {}
    
    # # ë””ë²„ê¹…: ì´ ì½”ë“œê°€ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸
    # logger.info("ğŸ” [DEBUG] Orchestrator section reached")
    # 
    # try:
    #     from .orchestrator import orchestrator_llm, execute_tools
    #     from app.db.database import SessionLocal
    #     
    #     logger.info("=" * 60)
    #     logger.info("ğŸ¯ [PHASE 2] Orchestrator Starting...")
    #     logger.info("=" * 60)
    #     
    #     # Context for orchestrator
    #     context = {
    #         "session_id": session_id,
    #         "user_id": user_id,
    #         "memory": "",  # í•„ìš”ì‹œ ì¶”ê°€
    #         "history": store.get_history(user_id, session_id, limit=3)
    #     }
    #     
    #     # Call orchestrator LLM
    #     tool_calls = await orchestrator_llm(
    #         user_text=user_text,
    #         context=context
    #     )
    #     
    #     orchestrator_tools = [tc.function.name for tc in tool_calls]
    #     logger.warning(f"ğŸ¯ [PHASE 2] Tools selected: {orchestrator_tools}")
    #     
    #     # Execute tools (optional - í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ë§Œ)
    #     if tool_calls:
    #         db_session = SessionLocal()
    #         try:
    #             orchestrator_results = await execute_tools(
    #                 tool_calls, user_id, session_id, user_text, db_session
    #             )
    #             logger.warning(f"ğŸ¯ [PHASE 2] Tool results: {list(orchestrator_results.keys())}")
    #         finally:
    #             db_session.close()
    #     
    #     logger.warning("=\" * 60)
    #     logger.warning("ğŸ¯ [PHASE 2] Orchestrator Complete")
    #     logger.warning("=" * 60)
        
    # except Exception as e:
    #     logger.error(f"âŒ [PHASE 2] Orchestrator failed: {e}", exc_info=True)
    #     import traceback
    #     logger.error(f"Full traceback:\n{traceback.format_exc()}")
    
    # âš¡ Emotion analysis removed from here - moved to background after response
        
    # 3. Slow Track: Trigger Background Tasks (Routine, Memory Promotion)
    # We create a task and wait with a timeout (Hybrid Approach)
    slow_track_task = asyncio.create_task(
        run_slow_track(user_text, None, user_id, session_id)  # âš¡ No emotion_result yet
    )
    
    routine_result = []
    try:
        # Wait for routine recommendation with a timeout (e.g., 1.0s)
        # If it finishes, we get the result. If not, we proceed without it.
        # This balances "Fast Response" with "Rich Content".
        slow_track_result = await asyncio.wait_for(asyncio.shield(slow_track_task), timeout=1.0)
        routine_result = slow_track_result.get("routine_result", [])
        logger.info(f"ğŸ¢ [Slow Track] Finished within timeout. Items: {len(routine_result)}")
    except asyncio.TimeoutError:
        logger.warning(f"ğŸ¢ [Slow Track] Timed out (continuing in background)")
        # Task continues in background due to asyncio.shield
    except Exception as e:
        logger.error(f"ğŸ¢ [Slow Track] Error: {e}")

    # 4. Context Retrieval (Memory & RAG) - Kept in Fast Track for now for quality
    # Optimization: Could be parallelized with Emotion Analysis if refactored further
    memory_context = ""
    rag_context = ""
    
    try:
        # Memory Layer
        try:
            from .adapters.memory_adapter import get_memories_for_prompt
        except ImportError:
            from adapters.memory_adapter import get_memories_for_prompt
            
        memories = get_memories_for_prompt(session_id, user_id)
        if memories:
            memory_context = f"[ê¸°ì–µëœ ì •ë³´]\n{memories}\n"
            
        # RAG Layer
        try:
            from .conversation_rag_v2 import get_conversation_rag
            rag_store = get_conversation_rag()
            rag_store.add_message(user_id, session_id, "user", user_text)
            similar_msgs = rag_store.search_similar(user_id, user_text, session_id, k=3)
            if similar_msgs:
                rag_context = "[ê³¼ê±° ìœ ì‚¬ ëŒ€í™”]\n"
                for msg in similar_msgs:
                    rag_context += f"- {msg['role']}: {msg['content']} (session: {msg['session_id']})\n"
        except Exception as e:
            logger.error(f"RAG Error: {e}")
            
    except Exception as e:
        logger.error(f"Context Retrieval Error: {e}")
        
    # 5. Generate Response (Fast Track)
    conversation_history = store.get_history(user_id, session_id, limit=None)
    
    # ğŸ†• Phase 4: LLM ì‘ë‹µ ìƒì„± (clean text + audio tags + emotion)
    ai_response_dict = generate_llm_response(
        user_text=user_text,
        emotion_result=None,  # âš¡ No emotion result - LLM uses its own understanding
        conversation_history=conversation_history,
        memory_context=memory_context,
        rag_context=rag_context,
        user_id=user_id
    )
    
    # ë‘ ê°€ì§€ ë²„ì „ + emotion ì¶”ì¶œ
    ai_response_text_clean = ai_response_dict["text_clean"]  # í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©
    ai_response_text_with_tags = ai_response_dict["text_with_tags"]  # TTSìš©
    ai_response_text_with_type_tag = ai_response_dict["text_with_type_tag"]  # ğŸ†• TYPE íƒœê·¸ í¬í•¨ (response_type ê°ì§€ìš©)
    llm_emotion = ai_response_dict["emotion"]  # LLMì´ ì§ì ‘ ê²°ì •í•œ ê°ì •
    
    # [DEBUG] ë‘ ë²„ì „ ëª¨ë‘ ë¡œê¹…
    logger.warning("=" * 80)
    logger.warning("ğŸ” [AUDIO TAGS DEBUG] Final Response Extraction")
    logger.warning(f"ğŸ“± CLEAN (Frontend/DB): {ai_response_text_clean}")
    logger.warning(f"ğŸ¤ TAGGED (TTS Engine): {ai_response_text_with_tags}")
    logger.warning(f"ğŸ­ EMOTION (from LLM): {llm_emotion}")
    logger.warning("=" * 80)
    
    # 6. Save AI Response (ì¡°ê±´ë¶€) - ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ ì €ì¥ (audio tag ì œê±°ë¨)
    if save_to_db:
        store.add_message(user_id, session_id, "assistant", ai_response_text_clean)
    
    # Update RAG with AI response (ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ ì €ì¥)
    try:
        if 'rag_store' in locals():
            rag_store.add_message(user_id, session_id, "assistant", ai_response_text_clean)
    except Exception as e:
        logger.error(f"RAG Save Error: {e}")
        
    logger.info(f"âœ… [DeepAgents] Response generated (clean): {ai_response_text_clean[:50]}...")
    
    # âš¡ Phase 3: Generate response-type and emotion
    response_metadata = {}
    try:
        from .response_generator import generate_response_type, parse_alarm_request, generate_emotion_parameter
        from datetime import datetime
        
        # ğŸ†• ê¸°ë³¸ response_type ê°ì§€ (TYPE íƒœê·¸ í¬í•¨ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        response_type = generate_response_type(ai_response_text_with_type_tag)
        logger.info(f"ğŸ“‹ [Response Type] Detected: {response_type}")

        
        # ğŸ†• Alarm ìš”ì²­ íŒŒì‹± (í•­ìƒ ì‹¤í–‰) - clean text ì‚¬ìš©
        logger.info(f"ğŸ” [Alarm Parser] Checking for alarm requests...")
        alarm_data = parse_alarm_request(
            user_text=user_text,
            llm_response=ai_response_text_clean,
            current_datetime=datetime.now()
        )
        logger.info(f"âœ… [Alarm Parser] Result: {alarm_data.get('response_type')} (count: {alarm_data.get('count', 0)})")
        
        # Alarmì´ë©´ response_type ë®ì–´ì“°ê¸°
        if alarm_data.get("response_type") == "alarm":
            response_type = "alarm"
            logger.info(f"ğŸ¯ [Response Type] Override to: alarm")
        
        # âš¡ Emotionì€ LLMì´ ì§ì ‘ ê²°ì • (ì¶”ê°€ API í˜¸ì¶œ ì—†ìŒ)
        emotion = llm_emotion
        logger.info(f"âœ¨ [Emotion] Using LLM decision: {emotion}")
        
        response_metadata = {
            "emotion": emotion,
            "response_type": response_type
        }
        
        # Alarm ì •ë³´ ì¶”ê°€
        if alarm_data.get("response_type") == "alarm":
            response_metadata["alarm_info"] = {
                "count": alarm_data["count"],
                "data": alarm_data["data"]
            }
            logger.info(f"âœ¨ [Alarm Info] Included in response: {response_metadata['alarm_info']}")
        
        logger.info(f"âœ¨ [Response Type] Final: {response_type}")
    except Exception as e:
        logger.error(f"Failed to generate response type: {e}", exc_info=True)
        response_metadata = {"emotion": "happiness", "response_type": "normal"}
        
    # âš¡ 6.5. Background Tasks: Full Emotion Analysis (for emotion reports only)
    async def background_tasks():
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê°ì • ë¶„ì„ ìˆ˜í–‰ (ì‘ë‹µ ì†ë„ì— ì˜í–¥ ì—†ìŒ)"""
        try:
            # Full emotion analysis (for emotion reports)
            logger.info("ğŸ” [Background] Starting full emotion analysis...")
            emotion_response = await run_fast_track(user_text, user_id=user_id)
            
            if emotion_response.get("skipped"):
                logger.info("â„¹ï¸  [Background] Full emotion analysis skipped")
                return
            
            emotion_result = emotion_response.get("result")
            if not emotion_result:
                return
                
            # Save to DB + ChromaDB cache (if fresh analysis)
            if not emotion_response.get("cached"):
                from sentence_transformers import SentenceTransformer
                import json
                
                embedder = SentenceTransformer('jhgan/ko-sroberta-multitask')
                embedding = embedder.encode(user_text).tolist()
                embedding_json = json.dumps(embedding)
                
                analysis_id = store.save_emotion_analysis(
                    user_id, user_text, emotion_result, 
                    check_root="conversation",
                    input_text_embedding=embedding_json
                )
                
                if analysis_id:
                    cache = get_emotion_cache()
                    cache.save(
                        user_id=user_id, input_text=user_text,
                        emotion_result=emotion_result, analysis_id=analysis_id
                    )
                    logger.info(f"ğŸ’¾ [Background] Saved: Analysis ID {analysis_id}")
        except Exception as e:
            logger.error(f"âŒ [Background] Background tasks failed: {e}")
    
    
    # âš ï¸ ë°±ê·¸ë¼ìš´ë“œ ê°ì •ë¶„ì„ ë¹„í™œì„±í™” (ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë¶„ë¦¬)
    # Frontendê°€ need_emotion_analysis=1ì¼ ë•Œ POST /emotion/api/analyze í˜¸ì¶œ
    # ğŸ’¾ Memory ManagerëŠ” TTS ì „ì— ì™„ë£Œë˜ë„ë¡ await ì‹¤í–‰ (TTS íƒ€ì„ì•„ì›ƒ ë°©ì§€)
    await run_slow_track(
        user_text=user_text,
        emotion_result=emotion,
        user_id=user_id,
        session_id=session_id
    )
    logger.info("ğŸš€ [Memory Manager] Background task created (run_slow_track)")
    logger.info("ğŸš€ [Endpoint Separation] Emotion analysis moved to /emotion/api/analyze")

    
    logger.info(f"âœ… [DeepAgents] Both text versions ready for return")

    
    # ğŸ†• Phase 4: ë‘ ê°€ì§€ ë²„ì „ì˜ í…ìŠ¤íŠ¸ ë°˜í™˜
    result = {
        "reply_text": ai_response_text_clean,  # í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš© (audio tag ì œê±°ë¨)
        "reply_text_with_tags": ai_response_text_with_tags,  # TTSìš© (audio tag í¬í•¨)
        "input_text": user_text,
        "emotion_result": None,  # âš¡ Analyzed in background
        "routine_result": routine_result,
        "emotion": response_metadata.get("emotion", "happiness"),  # ğŸ†• Phase 3
        "response_type": response_metadata.get("response_type", "normal"),  # ğŸ†• Phase 3
        "tts_audio": None,  # ğŸ†• Phase 2: TTS toggle (í˜„ì¬ëŠ” null, ì¶”í›„ êµ¬í˜„)
        "meta": {
            "model": os.getenv("OPENAI_MODEL_NAME", "gpt-4o-mini"),
            "session_id": session_id,
            "speaker_id": speaker_id,
            "memory_used": bool(memory_context),
            "rag_used": bool(rag_context),
            "stt_quality": stt_quality,
            # ğŸ†• Frontend compatibility: metaì—ë„ emotion/response_type í¬í•¨
            "emotion": response_metadata.get("emotion", "happiness"),
            "response_type": response_metadata.get("response_type", "normal")
        }
    }
    
    # ğŸ†• Alarm info í¬í•¨
    if "alarm_info" in response_metadata:
        result["alarm_info"] = response_metadata["alarm_info"]
        logger.info(f"âœ… [Return] alarm_info added to result: {response_metadata['alarm_info']}")
    
    # [DEBUG] ìµœì¢… API ì‘ë‹µ ë¡œê¹…
    logger.warning("=" * 80)
    logger.warning("ğŸ“¤ [AUDIO TAGS DEBUG] Final API Response")
    logger.warning(f"reply_text (clean): {result['reply_text']}")
    logger.warning(f"reply_text_with_tags: {result['reply_text_with_tags']}")
    logger.warning(f"emotion: {result.get('emotion')}")
    logger.warning(f"response_type: {result.get('response_type')}")
    logger.warning("=" * 80)
    
    return result

async def run_ai_bomi_from_audio_v2(
    audio_bytes: bytes,
    user_id: int,
    session_id: str = "default"
) -> dict[str, Any]:
    """
    ìŒì„± ì…ë ¥ ê¸°ë°˜ AI ë´„ì´ ì‹¤í–‰ (DeepAgents Prototype)
    """
    logger.info(f"ğŸ¤ [DeepAgents] Audio processing started (user_id: {user_id})")
    
    # 1. STT
    try:
        from .adapters import run_speech_to_text
    except ImportError:
        from adapters import run_speech_to_text
    
    stt_result = run_speech_to_text(audio_bytes)
    user_text = stt_result["text"]
    stt_quality = stt_result["quality"]
    
    if not user_text:
        return {
            "reply_text": "ì£„ì†¡í•´ìš”, ì˜ ë“¤ë¦¬ì§€ ì•Šì•˜ì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?",
            "input_text": "",
            "emotion_result": None,
            "routine_result": None,
            "meta": {
                "stt_quality": stt_quality,
                "session_id": session_id,
                "user_id": user_id,
                "storage": "database",
                "api_version": "v2_deepagents"
            }
        }
        
    # 2. Delegate to Text Handler
    return await run_ai_bomi_from_text_v2(user_text, user_id, session_id, stt_quality)
